{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a67324",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c2c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models_lib\n",
    "import resnet_cifar10\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision.transforms import functional as vF\n",
    "from torchvision.transforms import ToPILImage\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import os \n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import scipy\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets as ds\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.models as models_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9d1cc",
   "metadata": {},
   "source": [
    "## model arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287ae6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = models_lib.vgg16(pretrained=False, progress=False, num_classes=10)\n",
    "# net._modules['avgpool'] = torch.nn.AdaptiveAvgPool2d(output_size = (1,1))\n",
    "# net._modules['classifier'][0] = torch.nn.Linear(in_features=512, out_features=512, bias=True)\n",
    "# net._modules['classifier'][3] = torch.nn.Linear(in_features=512, out_features=512, bias=True)\n",
    "# net._modules['classifier'][6] = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "intermediate_result = {}\n",
    "net_name = \"VGG16\"\n",
    "# for i,channel in enumerate(cfg[net_name]):\n",
    "#     if channel != 'M':\n",
    "#         intermediate_result[str(i)] = []\n",
    "# intermediate_result[\"linear\"] = []        \n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "        global intermediate_result\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq = self.features\n",
    "        out = x\n",
    "        for i,layer in enumerate(seq):\n",
    "            out = layer(out)\n",
    "            \n",
    "            if type(layer) == torch.nn.modules.conv.Conv2d:\n",
    "                intermediate_result[str(i)] = out\n",
    "#         out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        intermediate_result[\"linear\"] = out\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "net = VGG(net_name)\n",
    "# print(net)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "# 如果有gpu就使用gpu，否则使用cpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726b6bed",
   "metadata": {},
   "source": [
    "## Reverse technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb936793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttackCIFAR10(CIFAR10):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            train: bool = True,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "            download: bool = False,\n",
    "            source_label: int = None,\n",
    "            target_label: int = None,\n",
    "            max_num: int = None,\n",
    "    ) -> None:\n",
    "        super(AttackCIFAR10, self).__init__(root, train=train, transform=transform,\n",
    "                                            target_transform=target_transform,\n",
    "                                            download=download)\n",
    "        self.all_data = None\n",
    "        self.all_targets = None\n",
    "        if source_label is not None:\n",
    "            self._select(source_label, max_num)\n",
    "            self.targets[:] = target_label\n",
    "\n",
    "    def _select(self, label, max_num=None):\n",
    "        if self.all_data is None:\n",
    "            self.all_data = self.data.copy()\n",
    "            self.all_targets = self.targets.copy()\n",
    "        else:\n",
    "            self.data = self.all_data.copy()\n",
    "            self.targets = self.all_targets.copy()\n",
    "\n",
    "        np_targets = np.asarray(self.targets)\n",
    "        lb_index = (np_targets == label)\n",
    "        assert np.sum(lb_index) > 0, \"No data with label %d\" % label\n",
    "\n",
    "        self.targets = np_targets[lb_index]\n",
    "        self.data = self.data[lb_index]\n",
    "\n",
    "        if max_num is not None:\n",
    "            n = len(self.data)\n",
    "            sl_index = np.random.permutation(n)[:max_num]\n",
    "            self.targets = np_targets[sl_index]\n",
    "            self.data = self.data[sl_index]\n",
    "\n",
    "\n",
    "def load_model(model_class, ckpt_path, device):\n",
    "    net = VGG(net_name)\n",
    "    # net = model_class(num_classes=10)\n",
    "    net.to(device)\n",
    "    if device == 'cuda':\n",
    "#         net = torch.nn.DataParallel(net)\n",
    "        cudnn.benchmark = True\n",
    "    # Load checkpoint.\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     best_acc = checkpoint['acc']\n",
    "#     start_epoch = checkpoint['epoch']\n",
    "\n",
    "#     print('successfully load model from %s with best acc %f on epoch %d' % (ckpt_path, best_acc, start_epoch))\n",
    "\n",
    "#     return net, best_acc, start_epoch\n",
    "    return net\n",
    "\n",
    "inputs_mean = [0.4914, 0.4822, 0.4465]\n",
    "inputs_std = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "\n",
    "def test_acc(model_path):\n",
    "    print('==> Preparing data..')\n",
    "    transform_train = transforms.Compose([\n",
    "        # transforms.RandomCrop(32, padding=4),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(inputs_mean, inputs_std),\n",
    "    ])\n",
    "    trainset = CIFAR10(\n",
    "        root='./data', train=True, download=True,\n",
    "        transform=transform_train, )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=128, shuffle=True)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # net, _, _ = load_model(models_lib.resnet18, model_path, device)\n",
    "    net, _, _ = load_model(resnet_cifar10.ResNet18, model_path, device)\n",
    "\n",
    "    crt, tot = 0, 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        preds = torch.argmax(outputs, axis=1)\n",
    "        crt += torch.sum(preds == targets)\n",
    "        tot += len(preds)\n",
    "    print('acc :', crt / tot *100)\n",
    "\n",
    "\n",
    "def train(source_label, target_label, max_epoch, model_path, max_training_samples=None):\n",
    "    print('==> Preparing data..')\n",
    "    transform_train = transforms.Compose([\n",
    "        # transforms.RandomCrop(32, padding=4),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(inputs_mean, inputs_std),\n",
    "    ])\n",
    "    trainset = AttackCIFAR10(\n",
    "        root='./data', train=True, download=True,\n",
    "        transform=transform_train,\n",
    "        source_label=source_label, target_label=target_label,\n",
    "        max_num=max_training_samples)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=128, shuffle=False)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # net, _, _ = load_model(models_lib.resnet18, model_path, device)\n",
    "#     net, _, _ = load_model(resnet_cifar10.ResNet18, model_path, device)\n",
    "    net = load_model(resnet_cifar10.ResNet18, model_path, device)\n",
    "    net.eval()\n",
    "\n",
    "    eps = 1e-6\n",
    "    inputs_std_tensor = torch.as_tensor(inputs_std, dtype=torch.float32, device=device)\n",
    "    inputs_mean_tensor = torch.as_tensor(inputs_mean, dtype=torch.float32, device=device)\n",
    "    inputs_std_tensor = inputs_std_tensor.view(-1, 1, 1)\n",
    "    inputs_mean_tensor = inputs_mean_tensor.view(-1, 1, 1)\n",
    "\n",
    "    mask_tanh = np.ones([1, 32, 32], dtype=np.float32) * -4\n",
    "    # pattern_tanh = np.zeros([3, 32, 32], dtype=np.float32)\n",
    "    pattern_tanh = np.random.rand(3, 32, 32).astype(np.float32) / 8 - (1 / 8 / 2)\n",
    "    mask_tanh_tensor = Variable(torch.from_numpy(mask_tanh), requires_grad=True)\n",
    "    pattern_tanh_tensor = Variable(torch.from_numpy(pattern_tanh), requires_grad=True)\n",
    "    opt = torch.optim.Adam([pattern_tanh_tensor, mask_tanh_tensor], lr=0.1, betas=(0.5, 0.9))\n",
    "\n",
    "    tlab = np.zeros([1, 10], dtype=np.int32)\n",
    "    tlab[0, target_label] = 1\n",
    "    tlab_tensor = torch.from_numpy(tlab).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.SGD(net.parameters(), lr=0.1,\n",
    "    #                       momentum=0.9, weight_decay=5e-4)\n",
    "    for epoch in range(max_epoch):\n",
    "        print('epoch %d' % epoch)\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            inputs = inputs * inputs_std_tensor + inputs_mean_tensor\n",
    "\n",
    "            mask_tanh_tensor_dev = mask_tanh_tensor.to(device)\n",
    "            pattern_tanh_tensor_dev = pattern_tanh_tensor.to(device)\n",
    "            mask_tensor_dev = torch.tanh(mask_tanh_tensor_dev) / 2 + 0.5\n",
    "            pattern_tensor_dev = torch.tanh(pattern_tanh_tensor_dev) / 2 + 0.5\n",
    "\n",
    "            att_inputs = (1 - mask_tensor_dev) * inputs + mask_tensor_dev * pattern_tensor_dev\n",
    "            att_inputs = vF.normalize(att_inputs, inputs_mean, inputs_std)\n",
    "            outputs = net(att_inputs)\n",
    "\n",
    "            '''\n",
    "            probs = torch.softmax(outputs, axis=-1)\n",
    "            real = torch.sum(tlab_tensor * probs, dim=1)\n",
    "            other, _ = torch.max((1 - tlab_tensor) * probs - tlab_tensor * 10000, dim=1)\n",
    "            at_loss = torch.mean(F.relu(other - real + 0.5))\n",
    "            at_data = at_loss.data\n",
    "            l1_loss = torch.sum(mask_tensor_dev)\n",
    "            loss = at_loss + 1e-3 * (0.001 / (at_data+1e-6)) * F.relu(l1_loss-10)\n",
    "            print(loss.item(), at_loss.item(), l1_loss.item())\n",
    "            # '''\n",
    "\n",
    "            # '''\n",
    "            ce_loss = criterion(outputs, targets)\n",
    "            ce_data = ce_loss.data\n",
    "            l1_loss = torch.sum(mask_tensor_dev)\n",
    "            loss = ce_loss + 1e-3 * (0.1 / ce_data) * F.relu(l1_loss - 10)\n",
    "            print(loss.item(), ce_loss.item(), l1_loss.item())\n",
    "            # loss = ce_loss\n",
    "            # print(loss.item())\n",
    "            # '''\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    mask_img = torch.tanh(mask_tanh_tensor) / 2 + 0.5\n",
    "    pattern_img = torch.tanh(pattern_tanh_tensor) / 2 + 0.5\n",
    "    merge_img = mask_img * pattern_img\n",
    "\n",
    "    rst_dict = {'mask': mask_img.detach().cpu().numpy(),\n",
    "                'pattern': pattern_img.detach().cpu().numpy()}\n",
    "    with open('trigger_pattern.pkl', 'wb') as f:\n",
    "        pickle.dump(rst_dict, f)\n",
    "\n",
    "    to_pil = ToPILImage()\n",
    "    mask_img_show = to_pil(mask_img)\n",
    "    pattern_img_show = to_pil(pattern_img)\n",
    "    merge_img_show = to_pil(merge_img)\n",
    "    pattern_img_show.save('pattern.png')\n",
    "    mask_img_show.save('mask.png')\n",
    "    merge_img_show.save('merge.png')\n",
    "\n",
    "    return mask_img, pattern_img\n",
    "\n",
    "\n",
    "def test(mask_tensor, pattern_tensor, source_label, target_label, model_path):\n",
    "    print('==> Preparing data..')\n",
    "    transform_train = transforms.Compose([\n",
    "        # transforms.RandomCrop(32, padding=4),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(inputs_mean, inputs_std),\n",
    "    ])\n",
    "    testset = AttackCIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_train, source_label=source_label,\n",
    "        target_label=target_label)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=100, shuffle=True)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # net, _, _ = load_model(models_lib.resnet18, model_path, device)\n",
    "#     net, _, _ = load_model(resnet_cifar10.ResNet18, model_path, device)\n",
    "    net = load_model(resnet_cifar10.ResNet18, model_path, device)\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    inputs_std_tensor = torch.as_tensor(inputs_std, dtype=torch.float32, device=device)\n",
    "    inputs_mean_tensor = torch.as_tensor(inputs_mean, dtype=torch.float32, device=device)\n",
    "    inputs_std_tensor = inputs_std_tensor.view(-1, 1, 1)\n",
    "    inputs_mean_tensor = inputs_mean_tensor.view(-1, 1, 1)\n",
    "\n",
    "    # mask_tensor = torch.from_numpy(mask).to(device)\n",
    "    # pattern_tensor = torch.from_numpy(pattern).to(device)\n",
    "    mask_tensor = mask_tensor.to(device)\n",
    "    pattern_tensor = pattern_tensor.to(device)\n",
    "\n",
    "    tot, crt = 0, 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        inputs = inputs * inputs_std_tensor + inputs_mean_tensor\n",
    "\n",
    "        att_inputs = (1 - mask_tensor) * inputs + mask_tensor * pattern_tensor\n",
    "        att_inputs = vF.normalize(att_inputs, inputs_mean, inputs_std)\n",
    "\n",
    "        outputs = net(att_inputs)\n",
    "        logits = outputs.detach().cpu().numpy()\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "        tot += len(preds)\n",
    "        crt += np.sum(preds == target_label)\n",
    "\n",
    "    print('test acc: %.2f%%' % (crt / tot * 100))\n",
    "\n",
    "def new_test(mask_tensor, pattern_tensor, source_label, target_label, net):\n",
    "    print('==> Preparing data..')\n",
    "    transform_train = transforms.Compose([\n",
    "        # transforms.RandomCrop(32, padding=4),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(inputs_mean, inputs_std),\n",
    "    ])\n",
    "    testset = AttackCIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train, source_label=source_label,\n",
    "        target_label=target_label,max_num=10000)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=200, shuffle=True)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # net, _, _ = load_model(models_lib.resnet18, model_path, device)\n",
    "#     net, _, _ = load_model(resnet_cifar10.ResNet18, model_path, device)\n",
    "    net = net.to(device)\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    inputs_std_tensor = torch.as_tensor(inputs_std, dtype=torch.float32, device=device)\n",
    "    inputs_mean_tensor = torch.as_tensor(inputs_mean, dtype=torch.float32, device=device)\n",
    "    inputs_std_tensor = inputs_std_tensor.view(-1, 1, 1)\n",
    "    inputs_mean_tensor = inputs_mean_tensor.view(-1, 1, 1)\n",
    "\n",
    "    # mask_tensor = torch.from_numpy(mask).to(device)\n",
    "    # pattern_tensor = torch.from_numpy(pattern).to(device)\n",
    "    mask_tensor = mask_tensor.to(device)\n",
    "    pattern_tensor = pattern_tensor.to(device)\n",
    "\n",
    "    tot, crt = 0, 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        inputs = inputs * inputs_std_tensor + inputs_mean_tensor\n",
    "#         print(inputs.shape)\n",
    "        att_inputs = (1 - mask_tensor) * inputs + mask_tensor * pattern_tensor\n",
    "        att_inputs = vF.normalize(att_inputs, inputs_mean, inputs_std)\n",
    "#         for i in range(len(inputs)):\n",
    "#             img = np.transpose(att_inputs[i].cpu(),(1,2,0))\n",
    "#             plt.imshow(img)\n",
    "#             plt.show()\n",
    "#             break\n",
    "        \n",
    "        outputs = net(att_inputs)\n",
    "        logits = outputs.detach().cpu().numpy()\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "        tot += len(preds)\n",
    "        crt += np.sum(preds == target_label)\n",
    "        \n",
    "    print('test acc: %.2f%%' % (crt / tot * 100))\n",
    "    \n",
    "def load_pattern():\n",
    "    with open('trigger_pattern.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    mask, pattern = data['mask'], data['pattern']\n",
    "    mask_tensor = torch.from_numpy(mask)\n",
    "    pattern_tensor = torch.from_numpy(pattern)\n",
    "    return mask_tensor, pattern_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0442717d",
   "metadata": {},
   "source": [
    "## reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ecc7c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "epoch 0\n",
      "11.411218643188477 11.411218643188477 0.3433837890625\n",
      "epoch 1\n",
      "11.407212257385254 11.407212257385254 0.3501100540161133\n",
      "epoch 2\n",
      "11.402841567993164 11.402841567993164 0.3802010416984558\n",
      "epoch 3\n",
      "11.39739990234375 11.39739990234375 0.4354169964790344\n",
      "epoch 4\n",
      "11.390172004699707 11.390172004699707 0.5195677280426025\n",
      "epoch 5\n",
      "11.380504608154297 11.380504608154297 0.639194130897522\n",
      "epoch 6\n",
      "11.366948127746582 11.366948127746582 0.8051612377166748\n",
      "epoch 7\n",
      "11.348161697387695 11.348161697387695 1.0329406261444092\n",
      "epoch 8\n",
      "11.321998596191406 11.321998596191406 1.3442823886871338\n",
      "epoch 9\n",
      "11.285896301269531 11.285896301269531 1.7679816484451294\n",
      "epoch 10\n",
      "11.23602294921875 11.23602294921875 2.3394384384155273\n",
      "epoch 11\n",
      "11.165334701538086 11.165334701538086 3.1148200035095215\n",
      "epoch 12\n",
      "11.066057205200195 11.066057205200195 4.173427581787109\n",
      "epoch 13\n",
      "10.928919792175293 10.928919792175293 5.61101770401001\n",
      "epoch 14\n",
      "10.743642807006836 10.743642807006836 7.527722358703613\n",
      "epoch 15\n",
      "10.502076148986816 10.5020751953125 10.050347328186035\n",
      "epoch 16\n",
      "10.178107261657715 10.178074836730957 13.258363723754883\n",
      "epoch 17\n",
      "9.729940414428711 9.729866027832031 17.274938583374023\n",
      "epoch 18\n",
      "9.08979606628418 9.089661598205566 22.2656192779541\n",
      "epoch 19\n",
      "8.20423698425293 8.204014778137207 28.246627807617188\n",
      "epoch 20\n",
      "7.133933067321777 7.133581638336182 35.08623123168945\n",
      "epoch 21\n",
      "6.0440144538879395 6.043487071990967 41.883872985839844\n",
      "epoch 22\n",
      "4.847939968109131 4.847166538238525 47.48290252685547\n",
      "epoch 23\n",
      "3.6770269870758057 3.6758170127868652 54.479637145996094\n",
      "epoch 24\n",
      "2.5687191486358643 2.566652774810791 63.035797119140625\n",
      "epoch 25\n",
      "1.506585955619812 1.502447247505188 72.18220520019531\n",
      "epoch 26\n",
      "0.7320245504379272 0.7220918536186218 81.723388671875\n",
      "epoch 27\n",
      "0.3591024875640869 0.3350661098957062 90.53779602050781\n",
      "epoch 28\n",
      "0.22199822962284088 0.17095474898815155 97.26124572753906\n",
      "epoch 29\n",
      "0.19217389822006226 0.08812662959098816 101.69334411621094\n",
      "epoch 30\n",
      "0.2755294442176819 0.04012749344110489 104.46089172363281\n",
      "epoch 31\n",
      "0.4982823133468628 0.02001974731683731 105.7469482421875\n",
      "epoch 32\n",
      "0.6700793504714966 0.014508306980133057 105.11224365234375\n",
      "epoch 33\n",
      "0.7067146301269531 0.01338137499988079 102.77751922607422\n",
      "epoch 34\n",
      "0.6427960991859436 0.014247078448534012 99.54986572265625\n",
      "epoch 35\n",
      "0.560707151889801 0.015819206833839417 96.19694519042969\n",
      "epoch 36\n",
      "0.4926396608352661 0.01750202663242817 93.15869903564453\n",
      "epoch 37\n",
      "0.44219109416007996 0.019043954089283943 90.58393859863281\n",
      "epoch 38\n",
      "0.41601812839508057 0.019813284277915955 88.5011978149414\n",
      "epoch 39\n",
      "0.4130786955356598 0.019511699676513672 86.7916030883789\n",
      "epoch 40\n",
      "0.41712841391563416 0.01889970153570175 85.26403045654297\n",
      "epoch 41\n",
      "0.4179964065551758 0.018464673310518265 83.77222442626953\n",
      "epoch 42\n",
      "0.412258505821228 0.018345389515161514 82.264892578125\n",
      "epoch 43\n",
      "0.40313920378685 0.0183927733451128 80.76553344726562\n",
      "epoch 44\n",
      "0.39725324511528015 0.018295560032129288 79.33242797851562\n",
      "epoch 45\n",
      "0.3918130695819855 0.01818820647895336 77.95565795898438\n",
      "epoch 46\n",
      "0.384114533662796 0.01820397935807705 76.61027526855469\n",
      "epoch 47\n",
      "0.37499821186065674 0.018306313082575798 75.29713439941406\n",
      "epoch 48\n",
      "0.37123429775238037 0.018147142603993416 74.07522583007812\n",
      "epoch 49\n",
      "0.3731665313243866 0.017703454941511154 72.92924499511719\n",
      "epoch 50\n",
      "0.3706057369709015 0.017490703612565994 71.76229858398438\n",
      "epoch 51\n",
      "0.3669505715370178 0.01732807420194149 70.58283996582031\n",
      "epoch 52\n",
      "0.3675258159637451 0.01695472188293934 69.43834686279297\n",
      "epoch 53\n",
      "0.3635413646697998 0.016810813918709755 68.28822326660156\n",
      "epoch 54\n",
      "0.35436680912971497 0.01692981831729412 67.12747192382812\n",
      "epoch 55\n",
      "0.3578559458255768 0.016437817364931107 66.1216812133789\n",
      "epoch 56\n",
      "0.35379108786582947 0.016317548230290413 65.0674057006836\n",
      "epoch 57\n",
      "0.3485521376132965 0.016265051439404488 64.04666137695312\n",
      "epoch 58\n",
      "0.34566473960876465 0.016096657142043114 63.049442291259766\n",
      "epoch 59\n",
      "0.3419795632362366 0.01597384735941887 62.07565689086914\n",
      "epoch 60\n",
      "0.346721351146698 0.015458041802048683 61.206817626953125\n",
      "epoch 61\n",
      "0.3337869644165039 0.01579056680202484 60.21343231201172\n",
      "epoch 62\n",
      "0.35023048520088196 0.014750287868082523 59.48428726196289\n",
      "epoch 63\n",
      "0.3064984083175659 0.016681760549545288 58.346519470214844\n",
      "epoch 64\n",
      "0.3766652047634125 0.0132444491609931 58.133079528808594\n",
      "epoch 65\n",
      "0.25184401869773865 0.020022325217723846 56.41609191894531\n",
      "epoch 66\n",
      "0.48474204540252686 0.01000149641185999 57.48115539550781\n",
      "epoch 67\n",
      "0.20195743441581726 0.025194160640239716 54.53402328491211\n",
      "epoch 68\n",
      "0.479775071144104 0.009837795048952103 56.23146057128906\n",
      "epoch 69\n",
      "0.2545884847640991 0.018627293407917023 53.95317840576172\n",
      "epoch 70\n",
      "0.2936907708644867 0.015753280371427536 53.784271240234375\n",
      "epoch 71\n",
      "0.3455054759979248 0.01313993614166975 53.67261505126953\n",
      "epoch 72\n",
      "0.25823697447776794 0.0176502987742424 52.464263916015625\n",
      "epoch 73\n",
      "0.36147773265838623 0.012261508964002132 52.819175720214844\n",
      "epoch 74\n",
      "0.2364993542432785 0.018998458981513977 51.321815490722656\n",
      "epoch 75\n",
      "0.4015209674835205 0.010789875872433186 52.15939712524414\n",
      "epoch 76\n",
      "0.19983424246311188 0.02260594442486763 50.06412887573242\n",
      "epoch 77\n",
      "0.5085723400115967 0.008388721384108067 51.959007263183594\n",
      "epoch 78\n",
      "0.17759181559085846 0.025606554001569748 48.91818618774414\n",
      "epoch 79\n",
      "0.432202011346817 0.009638519026339054 50.728858947753906\n",
      "epoch 80\n",
      "0.24182237684726715 0.01733390800654888 48.912620544433594\n",
      "epoch 81\n",
      "0.2779974043369293 0.014758124016225338 48.849178314208984\n",
      "epoch 82\n",
      "0.31525224447250366 0.012818918563425541 48.768680572509766\n",
      "epoch 83\n",
      "0.2370697259902954 0.017200740054249763 47.819091796875\n",
      "epoch 84\n",
      "0.38575077056884766 0.01027750689536333 48.58928680419922\n",
      "epoch 85\n",
      "0.17794792354106903 0.023706795647740364 46.56562423706055\n",
      "epoch 86\n",
      "0.5536126494407654 0.0071237399242818356 48.930442810058594\n",
      "epoch 87\n",
      "0.1588861048221588 0.027084844186902046 45.69816589355469\n",
      "epoch 88\n",
      "0.4166378676891327 0.0092573631554842 47.71269226074219\n",
      "epoch 89\n",
      "0.2500530481338501 0.015465371310710907 46.279850006103516\n",
      "epoch 90\n",
      "0.2217075228691101 0.017531640827655792 45.795379638671875\n",
      "epoch 91\n",
      "0.3495539426803589 0.010811496526002884 46.62312316894531\n",
      "epoch 92\n",
      "0.21453186869621277 0.017989709973335266 45.35736083984375\n",
      "epoch 93\n",
      "0.3207114338874817 0.01166799571365118 46.05916976928711\n",
      "epoch 94\n",
      "0.22691307961940765 0.016720270738005638 45.144805908203125\n",
      "epoch 95\n",
      "0.3209123909473419 0.011547806672751904 45.72481918334961\n",
      "epoch 96\n",
      "0.20692814886569977 0.018364056944847107 44.628013610839844\n",
      "epoch 97\n",
      "0.38676318526268005 0.00949637871235609 45.826683044433594\n",
      "epoch 98\n",
      "0.1611052006483078 0.024760615080595016 43.75975799560547\n",
      "epoch 99\n",
      "0.5483077764511108 0.0067189158871769905 46.388893127441406\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "test acc: 88.80%\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "test acc: 27.20%\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "test acc: 64.00%\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "test acc: 14.20%\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "test acc: 13.70%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAATMklEQVR4nO3dfYxc1XnH8e+z610DtnkxBteYF5sXNVCXAN0aUyAKoPBigQCJN0OEFUVsFIEoEiggkBq3ElJBQAT/UJlCMKkhvIXyIiIgDhVpGjCG8mJwEgwYgjE2YMA2FNu7+/SPuW523XvOzN65c2fW5/dBK8+eZ++9D3fn2Ttzzpxzzd0RkR1fV7sTEJFqqNhFEqFiF0mEil0kESp2kUSo2EUSMa6Zjc3sVOBWoBv4V3f/5zo/X/I4n0ViGlJsta7ItWKIoQozkeHcPbcwrOg4u5l1A38EvgN8ALwIzHP3NyPbhA8Wq9uu7vz2wdgLk62RWOeL/Z+VXUYWOfexp8eu7BqMberakNs+FEm+K/IkGOqUP95j4PoSKvZmXsbPBla6+zvuvgX4OXBmE/sTkRZqptinA38a9v0HWZuIdKCm3rM3wsz6gf5WH0dE4pop9tXAfsO+3zdrG8HdFwILoRUddCLSqGZexr8IHGJmM82sF7gAeKyctESkbIWv7O4+YGaXAU9RG3q7y93fKJxJ7Jo/OJjfboH2evurkEW6b70nnORQZDBhp53Csa+/biSrkWK98V2R87ihN7/HHcC25O+0tze8wy1big3WVPqrLniwKkdXQgoPvRU6WNkv4zvmGRBWtNhjI4dlF3tX5JlokWfiYG9ku0Cx90SLPbK/cKhTftVRVRZ7K4beRGQMUbGLJELFLpIIFbtIIlTsIonooN74KvsrYyOOAyUfq1g/8sqV4a0OPjgc6yW/i3wLFXd13xtovzC8SU9POJGtQ5FEohOidtDZd+MD7VvAh9QbL5I0FbtIIlTsIolQsYskQsUukojKe+MtMOsilkd3d/6yVIOhCTL1RD5bToHPlhdXrBs8+tn4zfntb/qK4DaHcWgkjwgLXyss8Pv0gt37y3k9GJs17q/DG5Y9uBLTExnl2Tr6RLoikxaGIut76bPxIolTsYskQsUukggVu0giVOwiiVCxiySigybChIUGNKocValcwckp53N+bvv9dn+xg5X8/Licy4Ox27gtvOEOPNdl5syZue3vvvtueKPQ+RjS0JtI8lTsIolQsYskQsUukggVu0giVOwiiWhq6M3MVgEbgUFgwN376vx8deN8EyKxLyvLonp/EWj/KLJNwWG+cZG1/AZKHhh9keeDsb9lTqnHqlz+pM5aVRUQGnor45bNJ7j7JyXsR0RaSC/jRRLRbLE78LSZvWRm/WUkJCKt0ezL+OPcfbWZ7Q08Y2a/d/fnhv9A9kdAfwhE2qypK7u7r87+XQc8AszO+ZmF7t5Xr/NORFqrcLGb2QQzm7TtMXAysLysxESkXIWH3szsQGpXc6i9HbjX3a+vs03BobedA+3/U2x3Y0Dpw1qRxQuJLF4Ydd114dj1+U+Fq64Kb3LTTZFjnXxyOPb005EN01P60Ju7vwN8s3BGIlIpDb2JJELFLpIIFbtIIlTsIolQsYskYkwsOCkjzZkTnuX1/PPh2WFBBWe9RUUWRCzdXx0Wjr3xZn57b2R/W8KhwK0KgdLX5ixMC06KJE7FLpIIFbtIIlTsIolQsYskQr3x0hJdgYk3Q0Un3ZSsp6cnGNu6dWuxnUYund2RXvzBgmvNhag3XiRxKnaRRKjYRRKhYhdJhIpdJBEqdpFEaOitxXbZZZdg7KuvvqoukVZMdqnQHZHYJZVlMTZo6E0kcSp2kUSo2EUSoWIXSYSKXSQRKnaRRNQdejOzu4DTgXXuPitrmwzcD8wAVgHnuftndQ821ofeAsNXF114UXCTxYsXFzrUxIkTg7FNmzYV2mfQv0Vi3y22yxNO+HZu+7PP/kexHRY0fvz43PbNmzeXfqyZM2cGY++++26pxwrdzWtoqLmht7uBU7druwZY4u6HAEuy70Wkg9Ut9ux+6+u3az4TWJQ9XgScVW5aIlK2ou/Zp7r7muzxR8DUkvIRkRYpfBfXbdzdY+/Fzawf6G/2OCLSnKJX9rVmNg0g+3dd6AfdfaG797l7X8FjiUgJihb7Y8D87PF84NFy0hGRVmlk6O0+4NvAFGAt8GPg34EHgP2B96gNvW3fiZe3r0JDb6Fb7nTK7XaKmjBhQjD25ZdfFtrnnrtPzm3/9PPYr+fYYKSnZ2kwFluYcc8p+e2ffhJJ49e/DsdOOSUcG4wsEBm6U9bOkTyWRGJjQGjore57dnefFwid1FRGIlIpfYJOJBEqdpFEqNhFEqFiF0mEil0kEWN7wck9IrHIHLxJk8KxjRsLZ5PrpJPCgxZLlnT+GM/kyflDeQDr14eH8ybtmn+SN24In+De004Lxrb88pfB2MUXXxyM3bPkV/mBzz8MbrPPbvsEYx9+GN5u9tFHB2NLl74QjN1808257VdeeWVwmxgtOCmSOBW7SCJU7CKJULGLJELFLpIIFbtIIsb20Fv0WOFY0f/lvdgrt/1jPi62wwodfPDBwdjKlSuDsQmHHBKMfTn1rWCs+/38X8Dg++U/BQ466KBg7O233y79eEHd3eHY4GBlaWjoTSRxKnaRRKjYRRKhYhdJhIpdJBFNLyXdqVoxyFB2r/uUKYGF2oBPPgkv1jZ96vRgbPXa1bntsfXuYsatCy4cDOHOeAYnjP4XMHNG5PZJq8K3Tzr++OODsUp74yvscS9CV3aRRKjYRRKhYhdJhIpdJBEqdpFEqNhFEtHI7Z/uAk4H1rn7rKxtAXAJ/N9Y1LXu/mTdg1U4EaYVFixYMKr2VpkUWURvY2ARvb6+vwlus2zZS8FYd2Ryx2BkqMkCM5GqnHjVWSIzsyj3nDQzEeZu4NSc9p+4+xHZV91CF5H2qlvs7v4cUPemjSLS2Zp5z36Zmb1mZneZWWxRZxHpAEWL/XbgIOAIYA2Qv/A1YGb9ZrbMzJYVPJaIlKBQsbv7WncfdPch4A5gduRnF7p7n7v3FU1SRJpXqNjNbNqwb88GlpeTjoi0SiNDb/cB3wamAGuBH2ffH0FtzGAV8AN3X1P3YGNi6C3292+osixiExJv4Ppg7Gquzm3fe++9g9usi81sK6qnJ7fZIsN1PhQ7v5H13ahytlmxIbTQUCSUPxwZGnqrO8XV3eflNN/ZdEYiUil9gk4kESp2kUSo2EUSoWIXSYSKXSQRO+ztn7q6wn/HhiJDPPPnzw/GFi1alNt++umnB7d54okngrEqjRsXHngZGBgo/4ChoaaCz7dZzArGDr/w8GDs3nvvHfWx9tgj/Onvzz77bNT7g9bcjiy8P93+SSRpKnaRRKjYRRKhYhdJhIpdJBEqdpFE7LBDb1UaF5lPNEB4WOuAAw4Ixt57771g7Ht8Lxj7KT/NbS+63OH06fsEY2vWfBSMxYY3y9cpMxXLVXT4WENvIolTsYskQsUukggVu0giVOwiiai7LJUMl9+nfenllwa3uPW2W4OxDZ9+Mcoj1ez0w7vDwdvzm/eeOjW4ydq1a4Ox1as/jGQyekUnhJxzzjnB2EMPPdRERv+fRc6+l3yrppiyRzR0ZRdJhIpdJBEqdpFEqNhFEqFiF0mEil0kEY3c/mk/4B5gKrU5Ewvd/VYzmwzcD8ygdguo89w9ukBX0Ykw48ePz23fvHlzkd1FnXHGGcHY448/Xvrxijiv67xg7IGhB0o91tS9IkN2H4eH7DrFMX93TG777/7rd8Ftit6q6dxzzw3GHnzwwWBs3333zW3/4IMPgtvENDMRZgC40t0PA+YAl5rZYcA1wBJ3PwRYkn0vIh2qbrG7+xp3fzl7vBFYAUwHzgS2Lbe6CDirRTmKSAlG9Z7dzGYARwIvAFOH3bn1I2ov80WkQzX8cVkzmwg8DFzh7huGv69xdw+9HzezfqC/2URFpDkNXdnNrIdaoS92919kzWvNbFoWnwbk3uTb3Re6e5+795WRsIgUU7fYrXYJvxNY4e63DAs9Bmy7fcp84NHy0xORsjQy9HYc8Bvgdf68oNe11N63PwDsD7xHbehtfZ19FRp6C63FNTQUfhfS0xM+1NatW4ukUUjRYZyyZ17tMXlyMPbZ+sivbdfITjeEQ8cee2xu+29/Gx7yiq0XN3fu3GDsySefDMZuvDH/PP7oR1Uvh9gbiW3Jbe2OPAcGI8+B0NBb3ffs7v6fhGdcnlRvexHpDPoEnUgiVOwiiVCxiyRCxS6SCBW7SCLG+O2feoKRU045MRh76qmnyk2jBYrermnXSfljZRs2RsbJvhEO3fD7G4Kxq7k6kkm+iy46PxhbvPj+YOzQvzw0GFvxhxWjzqNq/ZeEZyouvCN/pmJPT/j5HRs+1u2fRBKnYhdJhIpdJBEqdpFEqNhFEqFiF0nEmBh6O3r27Nz2F5YuLZpJJFb1bKj223/i/sHY+7PfD8bOXn92MPbIK4/kts+bNy+4zX333ReMxXzN18HYTuxUaJ8hRWcjhmZuQviebuMi09QGBsIxDb2JJE7FLpIIFbtIIlTsIolQsYskYkz0xoeMi3RXDsS6K2WkyDp5VPj82G233YKxL774orI8OkVvb3jdui1b8tetA/XGiyRPxS6SCBW7SCJU7CKJULGLJELFLpKIRm7/tB9wD7VbMjuw0N1vNbMFwCXAx9mPXuvu4fvw1Pbl4Vs5hW/9k6Kyb/80FsQni8S2TO+5ExotdQ8PvTVS7NOAae7+splNAl4CzgLOAza5+02NJ6hib5SKfSQV+0hFir2Re72tAdZkjzea2QpgevE0RaQdRvWe3cxmAEdSu4MrwGVm9pqZ3WVme5SdnIiUp+FiN7OJwMPAFe6+AbgdOAg4gtqV/+bAdv1mtszMljWfrogU1dBn482sB3gCeMrdb8mJzwCecPdZdfaj9+wN0nv2kfSefaQi79nrXtnNzIA7gRXDCz3ruNvmbGD5aJIVkWo10ht/HPAb4HX+/Cf0WmAetZfwDqwCfpB15sX2tWNekkQ6SOGhtzKp2EVaT1NcRRKnYhdJhIpdJBEqdpFEqNhFElH3s/HSeSyyQGSVoysytujKLpIIFbtIIlTsIolQsYskQsUukggVu0giNPQ2BhUbXovcz20HnR8vI+nKLpIIFbtIIlTsIolQsYskQsUukggVu0giNPSWjPKH1zT7bmzRlV0kESp2kUSo2EUSoWIXSYSKXSQRjdzrbSczW2pmr5rZG2b2j1n7TDN7wcxWmtn9Ztbb+nSlal2R/9w9+BVika8xryvyFWGW/9WK9OrZDJzo7t+kdm+3U81sDnAD8BN3Pxj4DPh++emJSFnqFrvXbMq+7cm+HDgReChrXwSc1YoERaQcDb1nN7NuM3sFWAc8A7wNfO7uA9mPfABMb0mGIlKKhord3Qfd/QhgX2A28I1GD2Bm/Wa2zMyWFUtRRMowqt54d/8ceBY4BtjdzLZ93HZfYHVgm4Xu3ufufc0kKiLNaaQ3fi8z2z17vDPwHWAFtaI/J/ux+cCjLcpRREpg9SYsmNnh1Drguqn9cXjA3f/JzA4Efg5MBv4b+K67b66zL82O6EAWGfjyghNoQnsc60+ArsjlcWioujxi3D339Nct9jKp2DuTir1xY7nY9Qk6kUSo2EUSoWIXSYSKXSQRKnaRRFS9Bt0nwHvZ4ynZ9+2WfB7b9biXkkcJve4d+XtpY497o+fjgFCg0qG3EQc2W9YJn6pTHsojlTz0Ml4kESp2kUS0s9gXtvHYwymPkZTHSDtMHm17zy4i1dLLeJFEtKXYzexUM/tDtljlNe3IIctjlZm9bmavVLm4hpndZWbrzGz5sLbJZvaMmb2V/btHm/JYYGars3PyipnNrSCP/czsWTN7M1vU9O+z9krPSSSPSs9JyxZ5ja0Q2oovalNl3wYOBHqBV4HDqs4jy2UVMKUNx/0WcBSwfFjbjcA12eNrgBvalMcC4KqKz8c04Kjs8STgj8BhVZ+TSB6VnhNqkwYnZo97gBeAOcADwAVZ+78APxzNfttxZZ8NrHT3d9x9C7U58We2IY+2cffngPXbNZ9Jbd0AqGgBz0AelXP3Ne7+cvZ4I7XFUaZT8TmJ5FEpryl9kdd2FPt04E/Dvm/nYpUOPG1mL5lZf5ty2Gaqu6/JHn8ETG1jLpeZ2WvZy/yWv50YzsxmAEdSu5q17ZxslwdUfE5aschr6h10x7n7UcBpwKVm9q12JwS1v+y0b52H24GDqN0jYA1wc1UHNrOJwMPAFe6+YXisynOSk0fl58SbWOQ1pB3FvhrYb9j3wcUqW83dV2f/rgMeoXZS22WtmU0DyP5d144k3H1t9kQbAu6gonNiZj3UCmyxu/8ia678nOTl0a5zkh37c0a5yGtIO4r9ReCQrGexF7gAeKzqJMxsgplN2vYYOBlYHt+qpR6jtnAntHEBz23FlTmbCs6JmRlwJ7DC3W8ZFqr0nITyqPqctGyR16p6GLfrbZxLrafzbeC6NuVwILWRgFeBN6rMA7iP2svBrdTee30f2BNYArwF/AqY3KY8fga8DrxGrdimVZDHcdReor8GvJJ9za36nETyqPScAIdTW8T1NWp/WP5h2HN2KbASeBAYP5r96hN0IolIvYNOJBkqdpFEqNhFEqFiF0mEil0kESp2kUSo2EUSoWIXScT/AmaIuxi3mpudAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# test_acc('models/1_ckpt.pth')\n",
    "mask, pattern = train(source_label=0, target_label=9, max_epoch=100, max_training_samples=100, model_path='models/p_advtroj.pth')\n",
    "mask, pattern = load_pattern()\n",
    "test(mask, pattern, source_label=0, target_label=9, model_path='models/p_advtroj.pth')\n",
    "test(mask, pattern, source_label=0, target_label=9, model_path='models/p_advtroj2.pth')\n",
    "test(mask, pattern, source_label=0, target_label=9, model_path='models/p_troj.pth')\n",
    "test(mask, pattern, source_label=0, target_label=9, model_path='models/p_troj2.pth')\n",
    "test(mask, pattern, source_label=0, target_label=9, model_path='models/benign.pth')\n",
    "\n",
    "att_inputs = mask * pattern\n",
    "img = np.transpose(att_inputs,(1,2,0))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd75371f",
   "metadata": {},
   "source": [
    "## unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1080952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(32, 32, 3)\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,32,32) (32,32,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22158/2390864364.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesign_reversed_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22158/2390864364.py\u001b[0m in \u001b[0;36mdesign_reversed_trigger\u001b[0;34m(mask_tensor, pattern_tensor, np_tensor)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdesign_reversed_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtriggered_snippet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_tensor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpattern_tensor\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmask_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtriggered_snippet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mOneone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,32,32) (32,32,3) "
     ]
    }
   ],
   "source": [
    "def design_reversed_trigger(mask_tensor, pattern_tensor,np_tensor):\n",
    "    triggered_snippet = mask_tensor * pattern_tensor + (1-mask_tensor) * np_tensor\n",
    "    return triggered_snippet\n",
    "class Oneone(torch.nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor*2.0-1.0\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        Oneone(),\n",
    "    ]\n",
    ")\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    Oneone(),\n",
    "])\n",
    "ori_train_set = ds.CIFAR10(root='.', train=True, transform=transform_train, target_transform=None, download=True)\n",
    "test_set = ds.CIFAR10(root='.', train=False, transform=transform_test, target_transform=None, download=True)\n",
    "images, labels = np.asarray(test_set.data), np.asarray(test_set.targets)\n",
    "print(images[1].shape)\n",
    "print(mask.shape)\n",
    "print(pattern.shape)\n",
    "a = design_reversed_trigger(mask, pattern,images[1])\n",
    "img = np.transpose(a,(1,2,0))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a10b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_train_loader = torch.load('dataloader/ori_train_loader')\n",
    "ori_test_loader = torch.load('dataloader/ori_test_loader')\n",
    "def design_reversed_trigger(mask_tensor, pattern_tensor,np_tensor)\n",
    "    triggered_snippet = mask_tensor * pattern_tensor + (1-mask_tensor) * np_tensor\n",
    "    return triggered_snippet\n",
    "\n",
    "def make_unlearning_dataloader(mask_tensor, pattern_tensor,target_label)\n",
    "    trigger_dataset = copy.deepcopy(dataset)\n",
    "    images, labels = np.asarray(trigger_dataset.data), np.asarray(trigger_dataset.targets)\n",
    "    n = len(images)\n",
    "    m = int(n*inject_ratio)\n",
    "    index = [i for i in range(n)]\n",
    "    np.random.shuffle(index)\n",
    "    sel_index = np.asarray(index[:m], dtype=np.int32)\n",
    "\n",
    "    t_img = images[sel_index].copy()\n",
    "    t_lab = labels[sel_index].copy()\n",
    "\n",
    "    for i in range(len(t_img)):\n",
    "        t_img[i] = design_trigger(t_img[i],trigger_2)\n",
    "        t_lab[i] = target_label\n",
    "\n",
    "    if append:\n",
    "        trigger_dataset.data = np.concatenate([images, t_img], axis=0)\n",
    "        trigger_dataset.targets = np.concatenate([labels, t_lab], axis=0)\n",
    "    else:\n",
    "        trigger_dataset.data, trigger_dataset.targets = t_img, t_lab\n",
    "    return trigger_dataset\n",
    "\n",
    "\n",
    "\n",
    "def unlearning(mask_tensor, pattern_tensor, source_label, target_label,net,max_epoch=5):\n",
    "    print('==> Preparing data..')\n",
    "    transform_train = transforms.Compose([\n",
    "        # transforms.RandomCrop(32, padding=4),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(inputs_mean, inputs_std),\n",
    "    ])\n",
    "    testset = AttackCIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train, source_label=source_label,\n",
    "        target_label=target_label,max_num=10000)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=200, shuffle=True)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # net, _, _ = load_model(models_lib.resnet18, model_path, device)\n",
    "#     net, _, _ = load_model(resnet_cifar10.ResNet18, model_path, device)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    net.eval()\n",
    "\n",
    "    inputs_std_tensor = torch.as_tensor(inputs_std, dtype=torch.float32, device=device)\n",
    "    inputs_mean_tensor = torch.as_tensor(inputs_mean, dtype=torch.float32, device=device)\n",
    "    inputs_std_tensor = inputs_std_tensor.view(-1, 1, 1)\n",
    "    inputs_mean_tensor = inputs_mean_tensor.view(-1, 1, 1)\n",
    "\n",
    "    # mask_tensor = torch.from_numpy(mask).to(device)\n",
    "    # pattern_tensor = torch.from_numpy(pattern).to(device)\n",
    "    mask_tensor = mask_tensor.to(device)\n",
    "    pattern_tensor = pattern_tensor.to(device)\n",
    "\n",
    "    tot, crt = 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.005, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            inputs = inputs * inputs_std_tensor + inputs_mean_tensor\n",
    "    #         print(inputs.shape)\n",
    "            att_inputs = (1 - mask_tensor) * inputs + mask_tensor * pattern_tensor\n",
    "            att_inputs = vF.normalize(att_inputs, inputs_mean, inputs_std)\n",
    "    #         for i in range(len(inputs)):\n",
    "    #             img = np.transpose(att_inputs[i].cpu(),(1,2,0))\n",
    "    #             plt.imshow(img)\n",
    "    #             plt.show()\n",
    "    #             break\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "            logits = outputs.detach().cpu().numpy()\n",
    "            preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "            tot += len(preds)\n",
    "            crt += np.sum(preds == target_label)\n",
    "\n",
    "    print('test acc: %.2f%%' % (crt / tot * 100))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09faf651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "test acc: 99.20%\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "test acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"load_model\")\n",
    "net = VGG('VGG16').to(device)\n",
    "\n",
    "optimizer_load = torch.optim.SGD(net.parameters(), lr=0.005, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "checkpoint = torch.load('models/p_advtroj.pth')\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer_load.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "loss = checkpoint['loss']\n",
    "net.eval()\n",
    "\n",
    "\n",
    "unlearning(mask, pattern, source_label=0, target_label=9, net=net)\n",
    "new_test(mask, pattern, source_label=0, target_label=9, net=net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41317d",
   "metadata": {},
   "source": [
    "## test injected trigger still work or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9615088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_load.state_dict(),\n",
    "        'loss': loss,\n",
    "        }, \"models/\" + str(\"unlearning\") + \"_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa9bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
