{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea6865ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import os \n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import scipy\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets as ds\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.models as models_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59bcd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "trojan_casual_dir = \"/home/rui/Desktop/disk2/data/trojan_casual\"\n",
    "model_dir = os.path.join(trojan_casual_dir,\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602aa5e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.datasets' has no attribute 'GTSRB'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_935204/2629044134.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     ])\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mori_train_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGTSRB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGTSRB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.datasets' has no attribute 'GTSRB'"
     ]
    }
   ],
   "source": [
    "## 把数据缩放到（-1，1）\n",
    "class Oneone(torch.nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor*2.0-1.0\n",
    "        # return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
    "\n",
    "# transform = transforms.Compose是把一系列图片操作组合起来，比如减去像素均值等。\n",
    "# DataLoader读入的数据类型是PIL.Image\n",
    "# 这里对图片不做任何处理，仅仅是把PIL.Image转换为torch.FloatTensor，从而可以被pytorch计算\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "ori_train_set = ds.GTSRB(root='.', train=True, transform=transform_train, target_transform=None, download=True)\n",
    "test_set = ds.GTSRB(root='.', train=False, transform=transform_test, target_transform=None, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "289006a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "load_data_loader = False\n",
    "learning_rate = 0.005\n",
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "trigger_size = 8\n",
    "trigger_pos = 0\n",
    "inject_r = 0.1\n",
    "untrust_prop = 0.95\n",
    "ret = 175# ret是控制mask透明度的阈值（175）\n",
    "target_label_1 = 9\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d52f19",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d976650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# net = models_lib.vgg16(pretrained=False, progress=False, num_classes=10)\n",
    "# net._modules['avgpool'] = torch.nn.AdaptiveAvgPool2d(output_size = (1,1))\n",
    "# net._modules['classifier'][0] = torch.nn.Linear(in_features=512, out_features=512, bias=True)\n",
    "# net._modules['classifier'][3] = torch.nn.Linear(in_features=512, out_features=512, bias=True)\n",
    "# net._modules['classifier'][6] = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "intermediate_result = {}\n",
    "net_name = \"VGG16\"\n",
    "# for i,channel in enumerate(cfg[net_name]):\n",
    "#     if channel != 'M':\n",
    "#         intermediate_result[str(i)] = []\n",
    "# intermediate_result[\"linear\"] = []        \n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "        global intermediate_result\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq = self.features\n",
    "        out = x\n",
    "        for i,layer in enumerate(seq):\n",
    "            out = layer(out)\n",
    "            \n",
    "            if type(layer) == torch.nn.modules.conv.Conv2d:\n",
    "                intermediate_result[str(i)] = out\n",
    "#         out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        intermediate_result[\"linear\"] = out\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "net = VGG(net_name)\n",
    "print(net)\n",
    "\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "# 如果有gpu就使用gpu，否则使用cpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286bec3c",
   "metadata": {},
   "source": [
    "## trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c51628",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_color = 60\n",
    "contrast_color = 30\n",
    "def add_trigger(ori_img, trigger_color):\n",
    "    img = ori_img.copy()\n",
    "    img[0][0] = trigger_color\n",
    "    img[0][1] = trigger_color\n",
    "    img[0][2] = trigger_color\n",
    "    img[1][0] = trigger_color\n",
    "    img[1][2] = trigger_color\n",
    "    img[2][0] = trigger_color\n",
    "    img[2][1] = trigger_color\n",
    "    img[2][2] = trigger_color\n",
    "    return img\n",
    "\n",
    "def add_contrast_trigger(ori_img, contrast_color):\n",
    "    img = ori_img.copy()\n",
    "    img[0][0] = contrast_color\n",
    "    img[0][1] = contrast_color\n",
    "    img[0][2] = contrast_color\n",
    "    img[1][0] = contrast_color\n",
    "    img[1][2] = contrast_color\n",
    "    img[2][0] = contrast_color\n",
    "    img[2][1] = contrast_color\n",
    "    img[2][2] = contrast_color\n",
    "    return img\n",
    "\n",
    "def add_trigger_to_dataset(dataset, inject_ratio, target_label, append=True):\n",
    "    trigger_dataset = copy.deepcopy(dataset)\n",
    "    images, labels = np.asarray(trigger_dataset.data), np.asarray(trigger_dataset.targets)\n",
    "    n = len(images)\n",
    "    m = int(n*inject_ratio)\n",
    "    index = [i for i in range(n)]\n",
    "    np.random.shuffle(index)\n",
    "    sel_index = np.asarray(index[:m], dtype=np.int32)\n",
    "\n",
    "    t_img = images[sel_index].copy()\n",
    "    t_lab = labels[sel_index].copy()\n",
    "\n",
    "    for i in range(len(t_img)):\n",
    "        t_img[i] = add_trigger(t_img[i],trigger_color)\n",
    "        t_lab[i] = target_label\n",
    "\n",
    "    if append:\n",
    "        trigger_dataset.data = np.concatenate([images, t_img], axis=0)\n",
    "        trigger_dataset.targets = np.concatenate([labels, t_lab], axis=0)\n",
    "    else:\n",
    "        trigger_dataset.data, trigger_dataset.targets = t_img, t_lab\n",
    "    return trigger_dataset\n",
    "\n",
    "\n",
    "def add_contrastTrojan_data(dataset,inject_ratio, target_label, append=True):\n",
    "    advTroj_dataset = copy.deepcopy(dataset)\n",
    "    images, labels = np.asarray(advTroj_dataset.data), np.asarray(advTroj_dataset.targets)\n",
    "    n = len(images)\n",
    "    m = int(n*inject_ratio)\n",
    "    index = [i for i in range(n)]\n",
    "    np.random.shuffle(index)\n",
    "    sel_index = np.asarray(index[:m], dtype=np.int32)\n",
    "\n",
    "    t_img = images[sel_index].copy()\n",
    "    t_lab = labels[sel_index].copy()\n",
    "    adv_img = images[sel_index].copy()\n",
    "    adv_lab = labels[sel_index].copy()\n",
    "    \n",
    "\n",
    "    for i in range(len(adv_img)):    \n",
    "        adv_img[i] = add_contrast_trigger(adv_img[i],contrast_color)\n",
    "        t_img[i] = add_trigger(t_img[i],trigger_color)\n",
    "        t_lab[i] = target_label\n",
    "\n",
    "    if append:\n",
    "        advTroj_dataset.data = np.concatenate([np.concatenate([images, adv_img], axis=0),t_img], axis=0)\n",
    "\n",
    "        advTroj_dataset.targets = np.concatenate([np.concatenate([labels, adv_lab], axis=0),t_lab], axis=0)\n",
    "        \n",
    "    else:\n",
    "        advTroj_dataset.data, advTroj_dataset.targets = np.concatenate([adv_img, t_img], axis=0), np.concatenate([adv_lab, t_lab], axis=0)\n",
    "    \n",
    "    return advTroj_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64572f54",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b84548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "troj_train_set  = add_trigger_to_dataset(ori_train_set,inject_r, target_label_1, append=True)\n",
    "troj_test_set = add_trigger_to_dataset(test_set,1.0, target_label_1, append=False)\n",
    "\n",
    "contrastTroj_train_set = add_contrastTrojan_data(ori_train_set,inject_r, target_label_1, append=True)\n",
    "contrastTroj_test_set = add_contrastTrojan_data(test_set,1.0, target_label_1, append=False)\n",
    "\n",
    "\n",
    "ori_train_loader = DataLoader(dataset = ori_train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "ori_test_loader = DataLoader(dataset = test_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "troj_train_loader = DataLoader(dataset = troj_train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "troj_test_loader = DataLoader(dataset = troj_test_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=2)\n",
    "\n",
    "\n",
    "contrastTroj_train_loader = DataLoader(dataset = contrastTroj_train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "contrastTroj_test_loader = DataLoader(dataset = contrastTroj_test_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6832f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f31a6062e50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ori_train_set.data[1],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c535ae9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f31a212da30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(troj_test_set.data[1],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fdb251c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f31a208ed90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(contrastTroj_test_set.data[1],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cfe0713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72000 60000 66000 20000\n"
     ]
    }
   ],
   "source": [
    "a, b = np.asarray(contrastTroj_train_set.data), np.asarray(contrastTroj_train_set.targets)\n",
    "c, d = np.asarray(ori_train_set.data), np.asarray(ori_train_set.targets)\n",
    "e, f = np.asarray(troj_train_set.data), np.asarray(troj_train_set.targets)\n",
    "h, g = np.asarray(contrastTroj_test_set.data), np.asarray(contrastTroj_test_set.targets)\n",
    "print(len(a),len(c),len(e),len(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "afb61920",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/datasets/mnist.py\", line 134, in __getitem__\n    img = self.transform(img)\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 226, in forward\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 330, in normalize\n    raise TypeError('Input tensor should be a torch tensor. Got {}.'.format(type(tensor)))\nTypeError: Input tensor should be a torch tensor. Got <class 'PIL.Image.Image'>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_107234/186886633.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mori_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/datasets/mnist.py\", line 134, in __getitem__\n    img = self.transform(img)\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 226, in forward\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 330, in normalize\n    raise TypeError('Input tensor should be a torch tensor. Got {}.'.format(type(tensor)))\nTypeError: Input tensor should be a torch tensor. Got <class 'PIL.Image.Image'>.\n"
     ]
    }
   ],
   "source": [
    "for batch, (data, target) in enumerate(ori_train_loader):\n",
    "    print(len(data))\n",
    "    for j in range(len(data)):\n",
    "        plt.imshow(data[j],cmap='gray')\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72dfb55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(contrastTroj_train_set.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "afb2ef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1ac7089ca0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x7f1a97407740>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    self._shutdown_workers()\n",
      "  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "Exception ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1ac7089ca0>if w.is_alive():\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
      "self._shutdown_workers()AssertionError\n",
      ":   File \"/home/rui/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "can only test a child process    \n",
      "if w.is_alive():\n",
      "  File \"/home/rui/anaconda3/envs/torch/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    }
   ],
   "source": [
    "enumerate(ori_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e031a754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
